{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroToAzureCognitiveService.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PgQCX9eTN_Vj",
        "uNxfQdSWyXCv",
        "e1PKd1KMZVNr",
        "LoZaiNmiv5Fn",
        "8wFhIv3ZwqcZ",
        "nJfgMw4SyAxK"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY31Ru8YsqBs",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to cognitive services on azure \n",
        "\n",
        "use state of the art cognitive services models...\n",
        "\n",
        "(...with some mouse clicks using Azure, Google Colabatory and a lot of code snippets).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgQCX9eTN_Vj",
        "colab_type": "text"
      },
      "source": [
        "## Setup Colabratory Environment:\n",
        "- installation of Azure CLI & python modules\n",
        "- import python modules\n",
        "- login to azure\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WasowSHwY5p9",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Install Azure CLI & python modules\n",
        "!curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
        "!pip install ffmpeg-python\n",
        "!pip install wavio\n",
        "!pip install pysoundfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlzYx2Csk4KJ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Import Python Packages\n",
        "from IPython.display import display, Javascript, HTML, Audio, Markdown, clear_output\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "from scipy.io.wavfile import write as wav_write\n",
        "import io\n",
        "import ffmpeg\n",
        "import wavio\n",
        "from PIL import Image, ImageDraw\n",
        "from six.moves import urllib\n",
        "import json\n",
        "import os, requests\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import time\n",
        "from xml.etree import ElementTree\n",
        "import soundfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F9BQQSxZM-l",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Login to Azure\n",
        "!az login"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNxfQdSWyXCv",
        "colab_type": "text"
      },
      "source": [
        "## Setup Azure Cognitive Services Resources\n",
        "\n",
        "Install cognitive services on azure:\n",
        "- ComputerVision\n",
        "- Face\n",
        "- LUIS\n",
        "- TextAnalytics\n",
        "- SpeechServices \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV70w65QZUDw",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Install cognitive services on azure via bash script\n",
        "%%bash\n",
        "resourceGroup=MyGroup\n",
        "# create resource group\n",
        "az group create -l westeurope -n $resourceGroup\n",
        "\n",
        "# create array with all the resources\n",
        "# If you with to add more: see all valid kinds with `az cognitiveservices account list-kinds` \n",
        "resources=( \n",
        "\"ComputerVision\"\n",
        "\"Face\"\n",
        "\"LUIS\"\n",
        "\"TextAnalytics\"\n",
        "\"SpeechServices\")\n",
        "\n",
        "# loop over resources and create them\n",
        "for i in \"${resources[@]}\"; do\n",
        "    echo Creating Resource for $i\n",
        "    echo $(az cognitiveservices account create --name $i --resource-group $resourceGroup \\\n",
        "           --kind $i --sku F0 --location WestEurope --yes)\n",
        "done\n",
        "\n",
        "# creating file where to save the results\n",
        "rm keys.py \n",
        "touch keys.py\n",
        "\n",
        "echo \"subscriptions = {\" > keys.py\n",
        "# for every resource find the key and append it to keys.py\n",
        "for i in \"${resources[@]}\"; do\n",
        "    echo \"Retrieving Key for Resource $i \"\n",
        "    echo -e \"\\t\\\"$i\\\"\": $(az cognitiveservices account keys list --name $i \\\n",
        "         --resource-group $resourceGroup | grep key1 | cut -d' ' -f 4) >> keys.py\n",
        "done && echo \"}\" >> keys.py && echo \"print(subscriptions)\" >> keys.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6HY5j4k2fl7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Import keys into jupyter notebook\n",
        "from keys import subscriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV6XZ71QKEOv",
        "colab_type": "text"
      },
      "source": [
        "## Deploy Utilities "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1PKd1KMZVNr",
        "colab_type": "text"
      },
      "source": [
        "### Description Utilities\n",
        "- `get_headers_body` : requests to azure cognitive services are defined with a headers and a body\n",
        "- `send_request` :  to establish a connection, send our request and retrieve the response from the servers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkks6ZhNmYQJ",
        "colab_type": "text"
      },
      "source": [
        "#### **`get_headers_body`**\n",
        "\n",
        "requests to azure cognitive services are defined with a `headers` and a `body`. \n",
        "\n",
        "We introduce a function `get_headers_body` and define the variables dynamically. The `headers` contains the authorization key for a resource and the type of data we will send. In the body, we will specify the data we want to send.\n",
        "\n",
        "\n",
        "In detail:\n",
        "- The `headers` of most of our requests is a dictionary  with two keys:\n",
        "  - `Content-Type`: specifies the type of  data to be sent (`\"application/json\"`, `\"application/octet-stream\"` etc.)\n",
        "  \n",
        "  - `Ocp-Apim-Subscription-Key`: specifies our **authorization key for the specific resource we will use**.\n",
        "  \n",
        "\n",
        "- The `body` depends on the `Content-Type` of the headers and is where we \"put\" or \"point\" the data to be sent.\n",
        "  - For  ``'application/json'`` we send a dictionary, for example:\n",
        "```python\n",
        "body = {\"url\" : \"<http://www.with_some_image.jpg>\"}\n",
        "```\n",
        " \n",
        "  - For  `\"application/octet-stream\"`, the body is made up of binary data. We use this to send **local files** to the cloud. For example:\n",
        "```python\n",
        "body =  open('<filepath>',  'rb')\n",
        "\n",
        "\n",
        "Example - local file:\n",
        "\n",
        "```python\n",
        "headers, body = get_headers_body(API='ComputerVision', data='path_to_img', localfile=True)\n",
        "\n",
        "```\n",
        "Our headers and body are now defined as:\n",
        "```python\n",
        "headers = {\"Content-Type\": \"application/octet-stream\",\n",
        "                    \"Ocp-Apim-Subscription-Key\": subscriptions['ComputerVision']}\n",
        "                    \n",
        "body = open('path_to_img', 'rb')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwt8i_jLglk0",
        "colab_type": "text"
      },
      "source": [
        "#### **`send_request`**\n",
        "We introduce a second function `send_request`, to establish a connection, send our request and retrieve the response from the servers.\n",
        "\n",
        "We use this to send our headers and body to an specific API endpoint and retrieve the full response from the server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoZaiNmiv5Fn",
        "colab_type": "text"
      },
      "source": [
        "### Deploy Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpluVRSQYxKd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Deploy utilities to notebook\n",
        "def get_headers_body(API, data, localfile=False):\n",
        "    '''Returns headers and body for the respective API using your resource key.\n",
        "    API [str]: refers to the resource name as in `subscriptions`\n",
        "    localfile [bool]: wether data is local or remote\n",
        "    data [str]:  if localfile==True use local path of the data. Else use an URL'''\n",
        "    \n",
        "    content_Types = ['application/json', 'application/octet-stream']\n",
        " \n",
        "    # Defines Body\n",
        "    if localfile == False:\n",
        "      body = \"{\"+'url: \"{}\"'.format(data)+\"}\"\n",
        "    else:\n",
        "      body = open(data, 'rb')\n",
        "    \n",
        "    # Defines Headers\n",
        "    headers = { 'Content-Type': content_Types[localfile],\n",
        "              'Ocp-Apim-Subscription-Key': subscriptions[API]}\n",
        "\n",
        "    return headers, body\n",
        "\n",
        "def send_request(endpoint, headers, body, params, location='westeurope'):\n",
        "  '''Sends headers, body and parameters to a given API and endpoint'''\n",
        "  try:\n",
        "    conn = http.client.HTTPSConnection(f'{location}.api.cognitive.microsoft.com')\n",
        "    conn.request(\"POST\", f\"{endpoint}\" + \"?%s\" % params, body, headers)\n",
        "    response = conn.getresponse()\n",
        "    data = response.read()\n",
        "    r = json.loads(data.decode())\n",
        "    conn.close()\n",
        "    return r \n",
        "  except Exception as e:\n",
        "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
        "    \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If7c_wBEQzPF",
        "colab_type": "text"
      },
      "source": [
        "## Deploy Actuators \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VwcV0uBQfmHk"
      },
      "source": [
        "### Description Actuators \n",
        "\n",
        "- `take_picture`: accepts a filepath to save the picture.\n",
        "- `record_audio`: accepts a filepath to record the audio file. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wFhIv3ZwqcZ",
        "colab_type": "text"
      },
      "source": [
        "### Deploy Acuators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgnJDqPLAR-k",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def take_picture(filename='photo.jpg'):\n",
        "  '''Takes picture and saves it as <filename>'''\n",
        "  quality=0.8\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "      }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "def record_audio(message, audiofile = 'audio.wav'):\n",
        "  '''Records audio and saves it as <audiofile>'''\n",
        "  \n",
        "  AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 16000, //chrome seems to ignore, always 48k\n",
        "    //mimeType : 'audio/webm;codecs=opus' ! -> changed to PCM\n",
        "    mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording...\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "});\n",
        "}\n",
        "});  \n",
        "</script>\n",
        "\"\"\"\n",
        "  if message != None:\n",
        "    _ = input(f'{message}')\n",
        "  else:\n",
        "    pass\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data=eval_js('data')\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "  \n",
        "  riff_chunk_size = len(output) - 8\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "  \n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "  print('Audio recorded and saved as {}'.format(audiofile))\n",
        "  _ = wavio.write(audiofile, audio, sr, sampwidth=3)\n",
        "  # converting to the right format for Cogntive Services Speech API\n",
        "  data, samplerate = soundfile.read(audiofile)\n",
        "  soundfile.write(audiofile, data, samplerate)\n",
        "  return audiofile\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGyvhOCVQ7JP",
        "colab_type": "text"
      },
      "source": [
        "## Deploy Effectors \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv-mV4wLol4-",
        "colab_type": "text"
      },
      "source": [
        "### Description Effectors \n",
        "\n",
        "correspondent effectors to reveal the data:\n",
        "\n",
        "- `show_picture`: receives a filepath or URL with an image.\n",
        "- `play_audio`: receives a filepath with the audio file and plays it out.\n",
        "- `draw_boxes`: draw bounding boxes on pictures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJfgMw4SyAxK",
        "colab_type": "text"
      },
      "source": [
        "###Deploy Effectors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GxBp7sfQ-AP",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "def show_picture(img='photo.jpg', localfile=False):\n",
        "  '''Displays the picture in <filename>'''\n",
        "  if localfile:\n",
        "    display(Image.open(img))\n",
        "  else:\n",
        "    response = requests.get(img)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    display(img)\n",
        "\n",
        "from IPython.display import Audio\n",
        "def play_audio(audiofile='audio.wav', autoplay=False):\n",
        "  return Audio(audiofile, autoplay=autoplay)\n",
        "\n",
        "\n",
        "def draw_show_boxes(img, regions, boxes, localfile=False):\n",
        "    if not localfile: # we need to download the image\n",
        "      r = requests.get(img, allow_redirects=True)\n",
        "      img = 'temp.jpg'\n",
        "      open(img, 'wb').write(r.content)\n",
        "    # Drawing boxes\n",
        "    im = Image.open(img)   \n",
        "    d = ImageDraw.Draw(im)\n",
        "    for i, label in zip(boxes, regions):\n",
        "    # `i` will need to represent the x-coordinate of the left edge, \n",
        "    # the y-coordinate of the top edge, width, and height of the bounding box\n",
        "      x, y, width, height = i\n",
        "      top, top_right = (x, y), (x+width, y)\n",
        "      bottom, bottom_right = (x, y+height), (x+width, y+height)\n",
        "      line_color = (0, 0, 255)\n",
        "      d.line([top, top_right, bottom_right, bottom, top], fill=line_color, width=2)\n",
        "      d.text(top,str(label),(0,0,255))\n",
        "    im.save('temp.jpg')\n",
        "    return show_picture('temp.jpg', localfile=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPF6yIQrrKCt",
        "colab_type": "text"
      },
      "source": [
        "# Cognitive Services Snippets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJiVYlBRqemz",
        "colab_type": "text"
      },
      "source": [
        "###Notes\n",
        "For each API, we will define a couple of functions and see practical examples of how to use them. \n",
        "\n",
        "pattern:\n",
        "\n",
        "- header, body and request parameters (params) are defined.\n",
        "- The request is sent and a response retrieved\n",
        "- The response is parsed to extract some useful information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY2LEFauKQIy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Computer Vision API\n",
        "\n",
        "> Extract rich information from images to categorize and process visual data\n",
        "\n",
        "We will implement four functions that talk to certain API endpoints (specified in `send_request`) that will allow us to analyze images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzho5e_jYFp9",
        "colab_type": "text"
      },
      "source": [
        "#### `describe`: \n",
        "* Returns one or more textual descriptions of an image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD2qWntDrDw4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _describe_\n",
        "def describe(img, number_of_descriptions=1, localfile=False):\n",
        "    '''Returns a number_of_descriptions from an image'''\n",
        "    params = urllib.parse.urlencode({\n",
        "    # Request parameters\n",
        "    'maxCandidates': \"{}\".format(number_of_descriptions),\n",
        "    'language': 'en'})\n",
        "    \n",
        "    # Defining body and headers\n",
        "    headers, body = get_headers_body(API='ComputerVision', data=img, localfile=localfile)\n",
        "    \n",
        "    # Send request and retrieve response\n",
        "    r = send_request('/vision/v2.0/describe', params=params, headers=headers, body=body)\n",
        "    \n",
        "    # Parse response to return the info of interest\n",
        "    description = '\\n' \n",
        "    for i in range(len(r['description']['captions'])):\n",
        "      description += r['description']['captions'][i]['text'].capitalize()+'\\n'\n",
        "    return description"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPuMixJP2Sof",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# showing image\n",
        "img = 'https://images.unsplash.com/photo-1563207153-f403bf289096?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=800&q=80'\n",
        "# showing and describing image\n",
        "show_picture(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvEg56bqkQ9_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# describing image\n",
        "description = describe(img, number_of_descriptions=3)\n",
        "print(description)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM0hXY4AgxnP",
        "colab_type": "text"
      },
      "source": [
        "#### `classify`\n",
        "\n",
        "* Assigns a category to an image and tags it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS3Kc79fgv9r",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _classify_\n",
        "def classify(img, localfile=False, visualFeatures='Description, Categories, Faces'):\n",
        "    '''visualFeatures: a string of comma separated keywords 'Brands, Adult, Objects'\n",
        "    Returns tags in an image\n",
        "    '''\n",
        "    params = urllib.parse.urlencode({\n",
        "      # Request parameters\n",
        "      'visualFeatures': '{}'.format(visualFeatures),\n",
        "      'details': '',\n",
        "      'language': 'en'})\n",
        "    \n",
        "    # Defining body and headers\n",
        "    headers, body = get_headers_body(API='ComputerVision', data=img, localfile=localfile)\n",
        "    # Send headers, body a params and retrieve respose\n",
        "    r = send_request(\"/vision/v2.0/analyze\", params=params, headers=headers, body=body)\n",
        "    \n",
        "    # Parse response to return the info of interest\n",
        "    info = '\\nCategories in Image: \\n'\n",
        "    for i in r['categories']:\n",
        "      info+=(\"\\t\"+ i['name'] +'\\n')\n",
        "    info+='Tags found in the image \\n\\t {}'.format(r['description']['tags'])\n",
        "    return info\n",
        "    if hasattr(img, 'close'):\n",
        "      img.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yFCU3KCldD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# showing picture\n",
        "img='https://images.unsplash.com/photo-1553196798-b71feabce946?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=800&q=80'\n",
        "show_picture(img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA8tvl2-jhAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# retrieving and printing info\n",
        "print(classify(img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHn_MLvzYMJh",
        "colab_type": "text"
      },
      "source": [
        "#### `read`: \n",
        "\n",
        "* Perform OCR and retrieve text from images.\n",
        "* Draws regions where text is located and displays the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmyRpJbBb7iF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _read_\n",
        "def read(img, localfile=False):\n",
        "  '''Retrieves text from images using OCR'''\n",
        "  global draw_show_boxes\n",
        "  # Request parameters\n",
        "  params = urllib.parse.urlencode({\n",
        "      'language': 'unk',\n",
        "      'detectOrientation': 'true'})\n",
        "  \n",
        "  # Defining body and headers\n",
        "  headers, body = get_headers_body(API='ComputerVision', data=img, localfile=localfile)\n",
        "  \n",
        "  # Send headers, body a params and retrieve respose\n",
        "  r = send_request('/vision/v2.0/ocr', params=params, headers=headers, body=body)\n",
        "  \n",
        "  # Parse response to return the info of interest: \n",
        "  boxes, texts = [], []# regions in the image\n",
        "  for i in r['regions']:\n",
        "    boxes.append([int(i) for i in i['boundingBox'].split(',')])\n",
        "    text = ''\n",
        "    for a in i['lines']:\n",
        "      text+=(' '.join([w['text'] for w in (a['words'])])+'\\n')\n",
        "    texts.append(text)\n",
        "\n",
        "  # draws rectangles surrounding the objects and info\n",
        "  IDs = ['Region'+str(i) for i in range(len(boxes))]\n",
        "  draw_show_boxes(img, IDs, boxes, localfile)\n",
        "  return texts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XclDsIgdWhi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# showing picture\n",
        "img= 'https://www.oreilly.com/library/view/handbook-of-information/9780471648307/images/titlepage.jpg'\n",
        "show_picture(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fosVYUfDkRp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# retrieving and printing text\n",
        "for line in read(img):\n",
        "  print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Vv_lvvsTmj",
        "colab_type": "text"
      },
      "source": [
        "#### `see` \n",
        "\n",
        "* Performs object detection and retrieves recognized objects and bounding boxes.\n",
        "* Draws and labels bounding boxes around the objects and displays result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4IoePqZrv95",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _see_\n",
        "def see(img, localfile=False):\n",
        "    '''Object Detection, displays the labeled bounding boxes in the image'''    \n",
        "    # Defining params, body and headers, send and retrieve response\n",
        "    params = urllib.parse.urlencode({})\n",
        "    headers, body = get_headers_body(API='ComputerVision', data=img, localfile=localfile)\n",
        "    r = send_request('/vision/v2.0/detect', params=params, headers=headers, body=body)\n",
        "    # manipulate response to extract useful information\n",
        "    print(f\"In the image of size {r['metadata']['width']} by {r['metadata']['height']} pixels, {len(r['objects'])} objects were detected\")\n",
        "                                                                               \n",
        "    found_objects, object_boxes = [], []\n",
        "    for i in r['objects']:\n",
        "        found_objects.append(i['object'])\n",
        "        print('{} detected at region {}'.format(i['object'], i['rectangle']))\n",
        "        box = []\n",
        "        for el in 'xywh':\n",
        "          box.append(i['rectangle'][el])\n",
        "        object_boxes.append(box)\n",
        "\n",
        "    # In this case, the response returns the recognized objects and their boundaring boxes.\n",
        "    # We can draw them in the image with the following function:\n",
        "    draw_show_boxes(img, found_objects, object_boxes, localfile)\n",
        "    return found_objects"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGEPTMxJZYPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "see('https://images.unsplash.com/photo-1493857671505-72967e2e2760?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=800&q=80')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBv6GHLTab2L",
        "colab_type": "text"
      },
      "source": [
        "### Face API\n",
        "\n",
        "> Detect faces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMxOcqtrKhX4",
        "colab_type": "text"
      },
      "source": [
        "#### `detect_face`\n",
        "\n",
        "* `detect_face`: detects faces and gives various information about them (including age, sex and sentiment). \n",
        "\n",
        "* `detect_objects` also returns boundaring boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v77CDCNEipUT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _detect_face_\n",
        "def detect_face(img, localfile=False):\n",
        "    \"\"\"Finds location and attributes of human faces in a picture (age, gender, emotion) and displays them\"\"\"\n",
        "    # Define headers, body and params, send and retrieve response\n",
        "    params = urllib.parse.urlencode({\n",
        "        'returnFaceId': 'true',\n",
        "        'returnFaceLandmarks': 'false',\n",
        "        'returnFaceAttributes': 'age,gender,smile,facialHair,glasses,emotion,hair',\n",
        "        'recognitionModel': 'recognition_01',\n",
        "        'returnRecognitionModel': 'false'})\n",
        "    headers, body = get_headers_body(API='Face', data=img, localfile=localfile)\n",
        "    r = send_request('/face/v1.0/detect', params=params, headers=headers, body=body)\n",
        "    # manipulate the response to extract information\n",
        "    boxes,genders,ages,emotions = [], [], [], []\n",
        "    for i in r:\n",
        "      boxes.append([i['faceRectangle'][el] for el in ['left', 'top', 'width', 'height']]) \n",
        "      genders.append(' '+i['faceAttributes']['gender'].capitalize())\n",
        "      ages.append(int(i['faceAttributes']['age']))\n",
        "      emotions = []\n",
        "      for i in r:\n",
        "        e, maxx = None, 0\n",
        "        for key, val in i['faceAttributes']['emotion'].items():\n",
        "          if val > maxx:\n",
        "            e, maxx = key, int(val*100)\n",
        "        emotions.append(f'{maxx}% {e}')\n",
        "    print(f'{len(boxes)} faces recognized')\n",
        "\n",
        "    # Draws object and writes information _ _ _ _ _ _ _\n",
        "    def draw_show_boxes(img, boxes, genders, ages, emotions, localfile=False):\n",
        "      if not localfile: # We need to download the image\n",
        "        r = requests.get(img, allow_redirects=True)\n",
        "        img = 'faces.jpg'\n",
        "        open(img, 'wb').write(r.content)\n",
        "      im = Image.open(img)   \n",
        "      # Drawing boxes\n",
        "      d = ImageDraw.Draw(im)\n",
        "      for i, gender, age, emotion in zip(boxes, genders, ages, emotions):\n",
        "        # represent the x-coordinate of the left edge, the y-coordinate of the top edge, width, and height of the bounding box\n",
        "        x, y, width, height = i\n",
        "        top, top_right = (x, y), (x+width, y)\n",
        "        bottom, bottom_right = (x, y+height), (x+width, y+height)\n",
        "        line_color = (0, 0, 255)\n",
        "        d.line([top, top_right, bottom_right, bottom, top], fill=line_color, width=2)\n",
        "        d.text(top,f'{gender} {age}',(0,0,255))\n",
        "        d.text(bottom, emotion,(0,0,255))\n",
        "      im.save(img)\n",
        "      return show_picture(img, localfile=True)\n",
        "\n",
        "    # draw boxes and writes info\n",
        "    return draw_show_boxes(img, boxes, genders, ages, emotions, localfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuWyUl73ix92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "r = detect_face('http://secureservercdn.net/198.71.233.19/3ec.e4c.myftpupload.com/wp-content/uploads/2014/04/Larremore-3.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcUYNxEkGogu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### realworld example: Capture and Send a Picture\n",
        "\n",
        "take a picture and analyse it with azure cognitive services. \n",
        "\n",
        "**_hint_**: Inside all of our functions, we can specify the argument `localfile=True` to allow us to send local files as binary images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRYXXTO9taV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# take a picture and save it as photo.jpg\n",
        "img = take_picture('photo.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-Vj_XvmG_ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# describe photo.jpg with 5 sentences\n",
        "print(describe(img, localfile=True, number_of_descriptions=5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5s7QHRYHAkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classify photo.jpg\n",
        "print(classify(img, localfile=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBixxXz6HBk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read and display any text \n",
        "print(read(img, localfile=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUQgxr18HCgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see the objects recognized \n",
        "see(img, localfile=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FTxiTk9HEHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# detect and display information about faces in the picture\n",
        "detect_face(img, localfile=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdALJdeHRO3c",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Text Analytics API\n",
        "\n",
        "> Used to analyze unstructured text for tasks such as sentiment analysis, key phrase extraction and language detection\n",
        "\n",
        "\n",
        "**NOTE**: This API requires us to define a different `body` with a list of dictionaries. Each one of them with the `text` to be analyzed and an `ID`. In our functions, we will construct this body using `*args` which will allow us to send multiple texts, all in one request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elOKmovvMZDv",
        "colab_type": "text"
      },
      "source": [
        "#### `detect_language` \n",
        "* Returns the language detected for each string in a list of strings. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G43hITcLG2FP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _detect_language_\n",
        "def detect_language(*documents):\n",
        "    '''Returns language detected for each string in documents (*args)'''\n",
        "    \n",
        "    params = urllib.parse.urlencode({}) \n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Ocp-Apim-Subscription-Key': subscriptions['TextAnalytics']}\n",
        "    \n",
        "    # redefine body for proper text format\n",
        "    body={\"documents\": []}\n",
        "    ID = 0\n",
        "    for document in documents:\n",
        "       doc = {\n",
        "            # assign unique idea\n",
        "            \"id\": ID,\n",
        "            \"text\": \"'{}'\".format(document)\n",
        "        }\n",
        "       body['documents'].append(doc)\n",
        "       ID+= 1\n",
        "    body = json.dumps(body)\n",
        "    \n",
        "    r = send_request('/text/analytics/v2.0/languages', params=params, headers=headers, body=body)\n",
        "    return  [i['detectedLanguages'][0]['name'] for i in  r['documents']]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-YqIety25mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# because of *args, our function can take a variable number of strings\n",
        "detect_language('Was soll das denn', 'No tengo ni idea', \"Don't look at me\", 'ごめんなさい', 'Sacré bleu!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEk3LHjHB-pr",
        "colab_type": "text"
      },
      "source": [
        "#### `key_phrases`\n",
        "* Returns the keys found in the given strings `documents`. *A list of strings denoting the key talking points in the input text*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JRhS7zmMjxn",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _key_phrases_\n",
        "def key_phrases(*documents):\n",
        "    # Define the parameters\n",
        "    params = urllib.parse.urlencode({})\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Ocp-Apim-Subscription-Key': subscriptions['TextAnalytics']}\n",
        "    body={\"documents\": []}\n",
        "    ID = 0\n",
        "    for document in documents:\n",
        "       doc = {\n",
        "            # assign unique idea\n",
        "            \"id\": ID,\n",
        "            \"text\": \"'{}'\".format(document)\n",
        "        }\n",
        "       body['documents'].append(doc)\n",
        "       ID+= 1\n",
        "    body = json.dumps(body)\n",
        "    r = send_request('/text/analytics/v2.0/keyPhrases', headers, body, params)\n",
        "    # _ _ _ _ _ _ _ \n",
        "    results = []\n",
        "    count = 0\n",
        "    for document in r['documents']:\n",
        "        doc = []\n",
        "        for phrase in document['keyPhrases']:\n",
        "          doc.append(f\"{phrase}\")\n",
        "        count += 1\n",
        "        results.append(doc)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mZFCcdAMrss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in key_phrases('I just spoke with the supreme leader of the galactic federation', 'I was dismissed', 'But I managed to steal the key', 'It was in his coat'):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyvFakcp2a_z",
        "colab_type": "text"
      },
      "source": [
        "#### `check_sentiment`\n",
        "\n",
        "* Assigns a positive, negative or neutral category to the strings given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttKpzJuU-Ult",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _check_sentiment_\n",
        "def check_sentiment(*documents):\n",
        "     # Define the parameters\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Ocp-Apim-Subscription-Key': subscriptions['TextAnalytics']}\n",
        "    params = urllib.parse.urlencode({})\n",
        "    ID = 0\n",
        "    body={\"documents\": []}\n",
        "    for document in documents:\n",
        "       doc = {\n",
        "            # assign unique idea\n",
        "            \"id\": ID,\n",
        "            \"text\": \"'{}'\".format(document)\n",
        "        }\n",
        "       body['documents'].append(doc)\n",
        "       ID+=1\n",
        "    body = json.dumps(body)\n",
        "    r = send_request('/text/analytics/v2.0/sentiment', headers, body, params)\n",
        "    # _ _ _ _ _ _ _ \n",
        "    sentiments = []\n",
        "    for document in r['documents']:\n",
        "      sentiment = \"negative\"\n",
        "      # if it's more than 0.5, consider the sentiment to be positive.\n",
        "      if document[\"score\"] >= 0.5:\n",
        "        sentiment = \"positive\"\n",
        "      elif document['score'] == 0.5:\n",
        "        sentiment = 'neutral'\n",
        "      sentiments.append(sentiment)\n",
        "    return sentiments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAF4mdgSACsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(check_sentiment('Not bad', \"Not good\", 'Good to know', 'Not bad to know', \n",
        "                      \"I didn't eat the hot dog\", 'Kill all the aliens'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz_UdBj95mBL",
        "colab_type": "text"
      },
      "source": [
        "#### `find_entities`\n",
        "\n",
        "* Returns a list of entities, assigned to a category. If possible, also a wikipedia link.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJpcDRLv6arl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _find_entities_\n",
        "def find_entities(*documents):  \n",
        "  \"\"\"Returns a list of entities recognized in each document of `documents`\"\"\"\n",
        "  headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Ocp-Apim-Subscription-Key': subscriptions['TextAnalytics']}\n",
        "  params = urllib.parse.urlencode({})\n",
        "  ID = 0\n",
        "  body={\"documents\": []}\n",
        "  for document in documents:\n",
        "      doc = {\n",
        "          \"id\": ID,\n",
        "          \"text\": \"'{}'\".format(document)\n",
        "      }\n",
        "      body['documents'].append(doc)\n",
        "      ID+=1\n",
        "  body = json.dumps(body)\n",
        "  r = send_request('/text/analytics/v2.1/entities', headers, body, params)\n",
        "  # _ _ _ _ _ _ _ \n",
        "  results = []\n",
        "  for doc in r['documents']:\n",
        "    names, types, links = [], [], []\n",
        "    for e in doc['entities']:\n",
        "      names.append(f\"{e['name']}, \")\n",
        "      types.append(f\"Type: {e['type']}, \")\n",
        "      try:\n",
        "        # if wikipedia score above a minimum a treshold append it:\n",
        "        if e['matches'][0]['wikipediaScore'] >= 0.10:\n",
        "          links.append(f\"Wiki [Link]({e['wikipediaUrl']})\")\n",
        "      except:\n",
        "        links.append(None)\n",
        "    docs = []\n",
        "    for n, t, l in zip(names, types, links):\n",
        "      if l == None:\n",
        "        doc_info= n + t\n",
        "      else:\n",
        "        doc_info = n + t + l\n",
        "      docs.append(doc_info)\n",
        "    results.append(docs)\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RNS67e87TKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "find_entities('I attended the lecture of Richard Feynmann at Cornell ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOBE9vIxTmke",
        "colab_type": "text"
      },
      "source": [
        "### realworld example: OCR + Text Analytics\n",
        "report function that extracts the individual text regions, analyzes them and makes up a report with our results:\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChDjwM28UqRY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _report_\n",
        "def report(img, localfile=False):\n",
        "  \"\"\"Recognizes and displays regions in the image with text.\n",
        "  Extracts text and analyzes its language, sentiment, key phrases and entities\n",
        "  Displays nicely in Markdown\"\"\"\n",
        "  # list of texts in all regions recognized with OCR\n",
        "  regions = read(img, localfile)\n",
        "  # list of languages for all regions\n",
        "  langs = detect_language(*regions)\n",
        "  # list of sentiments for all regions\n",
        "  sentiments = check_sentiment(*regions)\n",
        "  # list of entitites for all regions\n",
        "  entities = find_entities(*regions)\n",
        "  # list of key phrases for all regions\n",
        "  keys = key_phrases(*regions)\n",
        "\n",
        "  # looping over lists to print results. \n",
        "  m = ''\n",
        "  m +='# Report \\n'\n",
        "  count = 0\n",
        "  for r, l, s, e, k in zip(regions,langs, sentiments, entities, keys):\n",
        "    m += f'## Region {count}\\n'\n",
        "    m +='> \"'+r[0:82].rstrip()+'...'+'\"\\n\\n'\n",
        "    m+= f'- Language: {l}\\n'\n",
        "    m += f'- Sentiment: {s}\\n'\n",
        "    m += '- Entities:\\n'\n",
        "    for entity in e:\n",
        "      m += f' - {entity}\\n'\n",
        "    m += '- Keys:\\n'\n",
        "    for key in k:\n",
        "      m += f' - {key}\\n'\n",
        "    m += \"\\n\"\n",
        "    count += 1\n",
        "  #return display(Markdown(m)) -> for Jupyter\n",
        "  return print(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m8uEXIb2VPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report('https://i.pinimg.com/originals/5d/0c/90/5d0c90add4024dae1020e4e7fb545f7e.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF-2rYkySwa1",
        "colab_type": "text"
      },
      "source": [
        "##  Speech Services API\n",
        "\n",
        "> To convert audio to text and text-to-speech\n",
        "\n",
        "We will use it to transform voice to text and viceversa. This will allow us to hear and talk to our notebook. But before we can use Speech Services, we need to get a token (a secret string) that is valid for 10 minutes. For this we will use the function `get_token` as seen below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csEt5OvvUDng",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _get_token_\n",
        "def get_token(subscription_key):\n",
        "    '''Retrieves a token for Speech Services'''\n",
        "    global last_time, last_token\n",
        "    this_time = time.time()\n",
        "    fetch_token_url = 'https://westeurope.api.cognitive.microsoft.com/sts/v1.0/issueToken'\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': subscription_key\n",
        "    }\n",
        "    try:\n",
        "      # if more than 9.5 minutes have passed retrieve and return a new token\n",
        "      if (this_time - last_time)/60 >= 9.5:\n",
        "        response = requests.post(fetch_token_url, headers=headers)\n",
        "        new_token = str(response.text)\n",
        "        last_time = time.time()\n",
        "        last_token = new_token\n",
        "        print('9.5 minutes have passed, a new token has been retrieved')\n",
        "        return new_token\n",
        "      else:\n",
        "        #print('9.5 minutes have not passed, using last available token')\n",
        "        pass\n",
        "    except: # `last_time` does not exist, retrieve first token\n",
        "      last_time = time.time()\n",
        "      response = requests.post(fetch_token_url, headers=headers)\n",
        "      last_token = str(response.text)\n",
        "      # first token requested in session\n",
        "      print('First token retrieved')\n",
        "      return last_token\n",
        "    # last available token\n",
        "    return last_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3mOEXCbWGHa",
        "colab_type": "text"
      },
      "source": [
        "**Note**: token is valid for $10$ minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eYy4GV9eDkC",
        "colab_type": "text"
      },
      "source": [
        "### `speech_to_text`\n",
        "\n",
        "* To transform speech to text in [various languages](https://docs.microsoft.com/en-gb/azure/cognitive-services/speech-service/language-support#speech-to-text).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAutgudieGW5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _speech_to_text_\n",
        "def speech_to_text(path_to_wav, language='en-US'):\n",
        "    '''Input am audio .wav file, returns text with words recognized in the given language\n",
        "    See docs for valid arguments as language: https://docs.microsoft.com/en-gb/azure/cognitive-services/speech-service/language-support#speech-to-text'''\n",
        "    # opening audio file\n",
        "    with open(\"{}\".format(path_to_wav), mode=\"rb\") as audio_file:\n",
        "        audio_data =  audio_file.read()\n",
        "    \n",
        "    speechToTextEndPoint = \"https://westeurope.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1\"\n",
        "    headers = {\"Content-type\": \"audio/wav; codec=audio/pcm; samplerate=16000\", \n",
        "            \"Authorization\": \"Bearer \" + get_token(subscriptions['SpeechServices']),\n",
        "            \"Expect\":\"100-continue\"}\n",
        "    params = {\"language\":language}\n",
        "    body = audio_data\n",
        "\n",
        "    # Connect to server, post the request, and get the result\n",
        "    response = requests.post(speechToTextEndPoint,data=body, params=params, headers=headers)\n",
        "    result = str(response.text)\n",
        "    return json.loads(result)['DisplayText']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Vr24QJg2u9",
        "colab_type": "text"
      },
      "source": [
        "### `text_to_speech`\n",
        "\n",
        "* Now let's give voice to our notebook with the function `text_to_speech`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udD5DzX0hZfU",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define function _text_to_speech_\n",
        "def text_to_speech(text, path_to_wav='temp.wav'):\n",
        "  headers = {\n",
        "      \"X-Microsoft-OutputFormat\": 'riff-24khz-16bit-mono-pcm',\n",
        "      'Content-Type':'application/ssml+xml',\n",
        "      'Authorization': \"Bearer \" + get_token(subscriptions['SpeechServices']),\n",
        "      \"User-Agent\": \"SmartColabNotebook\"\n",
        "  }\n",
        "  endpoint = 'https://westeurope.tts.speech.microsoft.com/cognitiveservices/v1'\n",
        "  xml_body = ElementTree.Element('speak', version='1.0')\n",
        "  xml_body.set('{http://www.w3.org/XML/1998/namespace}lang', 'en-us')\n",
        "  voice = ElementTree.SubElement(xml_body, 'voice')\n",
        "  voice.set('{http://www.w3.org/XML/1998/namespace}lang', 'en-US')\n",
        "  voice.set('name', 'Microsoft Server Speech Text to Speech Voice (en-US, Guy24KRUS)')\n",
        "  voice.text = text\n",
        "  body = ElementTree.tostring(xml_body)\n",
        "\n",
        "  r = requests.post(endpoint, headers=headers, data=body)\n",
        "  if r.status_code == 200:\n",
        "    with open('temp.wav', 'wb') as audio:\n",
        "      audio.write(r.content)\n",
        "      #print(\"\\nStatus code: \" + str(r.status_code) + \"\\nVoice is ready for playback.\\n\")\n",
        "      return path_to_wav\n",
        "  return \"Voice could not be made\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnhxLO1MY32x",
        "colab_type": "text"
      },
      "source": [
        "Because `speech_to_text` receives an audio file and returns words and `text_to_speech` recieeves words and returns audio file, we can do something like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2os5ZcQwjDs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voice = text_to_speech(\"Hi, I'm your virtual assistant\")\n",
        "play_audio(voice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYNJ54a_XAVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "speech_to_text(voice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCayBDazY-Ju",
        "colab_type": "text"
      },
      "source": [
        "record voice with `record_audio`, transform it into words with `speech_to_text`, do something with the words and speak out loud the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThYxC7CCbP2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def motivational_bot():\n",
        "  '''Checks for feeling in your speech and says something'''\n",
        "  voice = record_audio('Press ENTER to record how you are feeling', 'voice.wav')\n",
        "  clear_output()\n",
        "  text = speech_to_text(voice)\n",
        "  sentiment = check_sentiment(text)[0]\n",
        "  if sentiment == 'negative':\n",
        "    voice = text_to_speech(\"Call 911, you are in deep trouble\")\n",
        "  else:\n",
        "    voice = text_to_speech(\"Nice to see you so positive today! Keep it up!\")\n",
        "  return play_audio(voice, autoplay=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsZ7WNy8GsGc",
        "colab_type": "text"
      },
      "source": [
        "start the flow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc8F2XCbgjBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "motivational_bot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hv32IzEmFFx",
        "colab_type": "text"
      },
      "source": [
        "Instead of checking for feelings in the words, we could translate them to a bunch of different languages (see the [Text Translator API](https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/) , look up for something on the web (see [Bing Search](https://dev.cognitive.microsoft.com/docs/services/f40197291cd14401b93a478716e818bf/operations/56b4447dcf5ff8098cef380d))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWA6JEJPbPp",
        "colab_type": "text"
      },
      "source": [
        "## Language Understanding\n",
        "> A machine learning-based service to build natural language understanding into apps, bots and IoT devices.\n",
        "\n",
        "Let's make our notebook more \"intelligent\" by giving it the ability to understand certain intentions in the language using the LUIS API. In short words, we will train a linguistic model that recognizes certain intentions in the language.\n",
        "\n",
        "For example, let's say we have the intent to `take_picture`. After training our model, if our notebook 'hears' sentences of the like of:\n",
        "- take a photo\n",
        "- use the camera and take a screenshot\n",
        "- take a pic\n",
        "\n",
        "It will know that our intention is to `take_picture`. We call these phrases **utterances**. And are what we need to provide to teach the language model how to recognize our **intents** (the tasks or actions we want to perform). \n",
        "\n",
        "\n",
        "By using varied and nonredundant utterances, as well as adding additional linguistic components such as entities and roles, you can create flexible and robust models tailored to your needs. Well implemented language models (backed up by the proper software) are what allow answer engines to respond questions like \"What is the weather in San Francisco?\", \"How many kilometers from Warsaw to Prag?\", \"How far is the Sun?\" etc.\n",
        "\n",
        "For this post though,  we will keep things simple and assign $5$ utterances to a handful of intents. As you might pressume, the intents will match some of the functions that we've already implemented. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR8vpCCNIYu8",
        "colab_type": "text"
      },
      "source": [
        "### Activate LUIS \n",
        "\n",
        "In contrast to all the services we've seen, LUIS is a complex tool that comes with its own \"Portal\", where you manage your LUIS apps and create, train, test and iteratively improve your models. But before we can  use it,we need to [activate a LUIS subscription](https://eu.luis.ai/). Once you've done this:\n",
        "\n",
        "- Go the LUIS dashboard and retrieve the **Authoring Key** of your account. If you need help with this step, take a look at the post or [refer here](https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-keys) for more detailed information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMos1YkRSFrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# paste your Authoring Key here (fake example)\n",
        "authoring_key = 'keykeykeykey'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn1nRqDXBMrg",
        "colab_type": "text"
      },
      "source": [
        "### Create a LUIS app\n",
        "\n",
        "The LUIS Portal makes it very easy to create your LUIS models, but we'll be using the [LUIS programmatic API](https://westus.dev.cognitive.microsoft.com/docs/services/5890b47c39e2bb17b84a55ff/operations/5890b47c39e2bb052c5b9c2f) to set things up from within the notebook using the `authoring_key`.\n",
        "\n",
        "- Let's start off by creating an app with the function [`create_luis_app`](https://westus.dev.cognitive.microsoft.com/docs/services/5890b47c39e2bb17b84a55ff/operations/5890b47c39e2bb052c5b9c2f):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8jU-WSlQs_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_luis_app(app_name):\n",
        "  headers = {\n",
        "      # Request headers\n",
        "      'Content-Type': 'application/json',\n",
        "      'Ocp-Apim-Subscription-Key': authoring_key}\n",
        "\n",
        "  luis_config = {\"name\": f\"{app_name}\",\n",
        "          \"culture\": \"en-us\",\n",
        "          \"description\": \"Language Understanding for Notebook\",\n",
        "          \"InitialVersionId\": \"1.0\",\n",
        "          }\n",
        "  body = json.dumps(luis_config)\n",
        "  params = urllib.parse.urlencode({\n",
        "  })\n",
        "  \n",
        "  r = send_request(\"/luis/api/v2.0/apps\", headers=headers, body=body, params=params)\n",
        "  return r, luis_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHGkRB-cLtGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app_id, luis_config = create_luis_app('NoteBot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trHEGmM7SYqy",
        "colab_type": "text"
      },
      "source": [
        "In this implementation, we keep track of the app ID (returned by the server) and the parameters that we specified inside `app_id` and `luis_config`  as global variables for later use.\n",
        "\n",
        "\n",
        "\n",
        "### Add intents and utterances\n",
        "Let's now define a function to add intents and a function to add their respective utterances.\n",
        "\n",
        "- `create_intent`: adds one intent to the luis app (specified by the variables `app_id` and `luis_config`).\n",
        "\n",
        "- `add_utterances`: adds a batch of examples/utterances to an existing intent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSYUE5AsV00a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_intent(intent):\n",
        "  headers = {\n",
        "      # Request headers\n",
        "      'Content-Type': 'application/json',\n",
        "      'Ocp-Apim-Subscription-Key': authoring_key,\n",
        "  }\n",
        "\n",
        "  params = urllib.parse.urlencode({\n",
        "  })\n",
        "\n",
        "  body = {\"name\": intent}\n",
        "  body = json.dumps(body)\n",
        "  try:\n",
        "      conn = http.client.HTTPSConnection('westeurope.api.cognitive.microsoft.com')\n",
        "      conn.request(\"POST\", f\"/luis/api/v2.0/apps/{app_id}/versions/{luis_config['InitialVersionId']}/intents?%s\" % params, body, headers)\n",
        "      response = conn.getresponse()\n",
        "      data = response.read()\n",
        "      print(data)\n",
        "      conn.close()\n",
        "  except Exception as e:\n",
        "      print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH0Oq9FrCP5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_utterances(intent, utterances):\n",
        "  headers = {\n",
        "      # Request headers\n",
        "      'Content-Type': 'application/json',\n",
        "      'Ocp-Apim-Subscription-Key': authoring_key,\n",
        "  }\n",
        "\n",
        "  params = urllib.parse.urlencode({\n",
        "  })\n",
        "\n",
        "  body = []\n",
        "  for utterance in utterances:\n",
        "    body.append({\"text\": utterance,\n",
        "                 \"intentName\": intent})\n",
        "  body = json.dumps(body)\n",
        "\n",
        "  try:\n",
        "      conn = http.client.HTTPSConnection('westeurope.api.cognitive.microsoft.com')\n",
        "      conn.request(\"POST\", f\"/luis/api/v2.0/apps/{app_id}/versions/{luis_config['InitialVersionId']}/examples?%s\" % params, body, headers)\n",
        "      response = conn.getresponse()\n",
        "      data = response.read()\n",
        "      print(data)\n",
        "      conn.close()\n",
        "  except Exception as e:\n",
        "      print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1CiLb58bBHY",
        "colab_type": "text"
      },
      "source": [
        "With these functions, let's define our language model inside a dictionary as seen below and apply them to it. There is big room for experimentation at this stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhgIh4mMg4iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a lot of room for experiment here\n",
        "intentions = {\n",
        "    \n",
        "'describe': [\"Can you describe what you see\",\n",
        "            \"Give a detailed account in words of the picture\",\n",
        "            \"Give me some descriptions about the image\",\n",
        "             \"Tell me something about the image\"], # -> speak\n",
        "              \n",
        "'see': [\"What objects can you detect\",\n",
        "        \"What objects do you recognize\",\n",
        "        \"What things can you percieve\"\n",
        "        \"Perform object detection\"],# -> Show and speak\n",
        "\n",
        "'detect_faces': [\"Do you see any humans\",\n",
        "                 \"Are there any men in the picture\"\n",
        "                 \"Show me the faces\",\n",
        "                 \"Do you detect any faces in the picture\"], # -> Show and Speak\n",
        "\n",
        "\"read\": ['What can you read',\n",
        "         \"Extract text from a picture\",\n",
        "         \"Can you read this out for me please\"\n",
        "         \"Read what you are seeing\"], # -> Speak\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHQGoJj9DALt",
        "colab_type": "text"
      },
      "source": [
        "The keys of this dictionary will be the intents for our application. Let's loop over them and create them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL_n4Gw_bMM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intents = intentions.keys()\n",
        "for intent in intents:\n",
        "  create_intent(intent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgqfqmsBC-CN",
        "colab_type": "text"
      },
      "source": [
        "Each intent has 4 examples/utterances, let's now add these to their respective intents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwhaDJUGgIxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for intent, utterances in intentions.items():\n",
        "  add_utterances(intent=intent, utterances=utterances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SMuX1tHkLIb",
        "colab_type": "text"
      },
      "source": [
        "*italicized text*## Train the model\n",
        "\n",
        "Let's now train the model with the information we've specified with [`train_luis_app`](https://westus.dev.cognitive.microsoft.com/docs/services/5890b47c39e2bb17b84a55ff/operations/5890b47c39e2bb052c5b9c45)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3e3PbOokNzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_luis_app(app_id, luis_config):\n",
        "  headers = {\n",
        "      # Request headers\n",
        "      'Ocp-Apim-Subscription-Key': authoring_key,\n",
        "  }\n",
        "  params = urllib.parse.urlencode({\n",
        "  })\n",
        "  body = \"\"\n",
        "  try:\n",
        "      conn = http.client.HTTPSConnection('westeurope.api.cognitive.microsoft.com')\n",
        "      conn.request(\"POST\", f\"/luis/api/v2.0/apps/{app_id}/versions/{luis_config['InitialVersionId']}/train?%s\" % params, body, headers)\n",
        "      response = conn.getresponse()\n",
        "      data = response.read()\n",
        "      print(data)\n",
        "      conn.close()\n",
        "  except Exception as e:\n",
        "      print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH3ulCXvl8kE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_luis_app(app_id, luis_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhiVPYHdnu-r",
        "colab_type": "text"
      },
      "source": [
        "### Publish the Application\n",
        "\n",
        "We are now ready to publish the application with [`publish_app`](https://westus.dev.cognitive.microsoft.com/docs/services/5890b47c39e2bb17b84a55ff/operations/5890b47c39e2bb052c5b9c3b)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMzQkudknxye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def publish_app(app_id):\n",
        "  headers = {\n",
        "      # Request headers\n",
        "      'Content-Type': 'application/json',\n",
        "      'Ocp-Apim-Subscription-Key': authoring_key,\n",
        "  }\n",
        "\n",
        "  params = urllib.parse.urlencode({\n",
        "  })\n",
        "\n",
        "  body = {\n",
        "      'versionId': luis_config['InitialVersionId'],\n",
        "      'isStaging': False,\n",
        "      'directVersionPublish': False\n",
        "  }\n",
        "  body = json.dumps(body)\n",
        "  try:\n",
        "      conn = http.client.HTTPSConnection('westeurope.api.cognitive.microsoft.com')\n",
        "      conn.request(\"POST\", f\"/luis/api/v2.0/apps/{app_id}/publish?%s\" % params, body, headers)\n",
        "      response = conn.getresponse()\n",
        "      data = response.read()\n",
        "      print(data)\n",
        "      conn.close()\n",
        "  except Exception as e:\n",
        "      print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWhw2mSAow3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# leave some time after training and\n",
        "publish_app(app_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMl6Dn8-m1ph",
        "colab_type": "text"
      },
      "source": [
        "### Making a prediction\n",
        "\n",
        "Let's see if our trained model is any useful  making predictions of our intents. Note that LUIS has a separate API to make predictions( the [LUIS endpoint API](https://westus.dev.cognitive.microsoft.com/docs/services/5819c76f40a6350ce09de1ac/operations/5819c77140a63516d81aee79)). Let's write the LUIS [prediction call](https://westus.dev.cognitive.microsoft.com/docs/services/5819c76f40a6350ce09de1ac/operations/5819c77140a63516d81aee78) as the function `understand`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhOPK6O3m0U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def understand(text, app_id=app_id):\n",
        "  headers = {\n",
        "      # Request headers\n",
        "      'Content-Type': 'application/json',\n",
        "      'Ocp-Apim-Subscription-Key': authoring_key,\n",
        "  }\n",
        "\n",
        "  params = urllib.parse.urlencode({\n",
        "      # Request parameters\n",
        "      'appId': app_id})\n",
        "\n",
        "  body = json.dumps(text)\n",
        "  try:\n",
        "      conn = http.client.HTTPSConnection('westeurope.api.cognitive.microsoft.com')\n",
        "      conn.request(\"POST\", f\"/luis/v2.0/apps/{app_id}?%s\" % params, body, headers)\n",
        "      response = conn.getresponse()\n",
        "      data = response.read()\n",
        "      data = json.loads(data)\n",
        "      conn.close()\n",
        "      try:\n",
        "        return data['topScoringIntent']['intent']\n",
        "      except:\n",
        "        return None\n",
        "  except Exception as e:\n",
        "      print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqfIRNkXqyT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "understand('can you describe the picture')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaqywIiDKcyk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "our notebook can understand what our intentions are from plain language. But having to type the text ourselves doesn't seem so helpful. The notebook should hear what we say and understand the intention that we have. Let's address this writing a function `hear` that uses `understand` together with the functions `record_audio` and `speech_to_text`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3qSl0ENrVt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hear():\n",
        "  \"\"\"Hears speech, transforms to text and predicts using the LUIS model specified by app_id\"\"\"\n",
        "  # starts recording\n",
        "  voice = record_audio(message=None, audiofile='voice.wav')\n",
        "  clear_output()\n",
        "  try:\n",
        "    text = speech_to_text(voice)\n",
        "  except:\n",
        "    print('Could not understand')\n",
        "    return \n",
        "  intent = understand(text)\n",
        "  if intent != None:\n",
        "    return intent\n",
        "\n",
        "def speak(text):\n",
        "    voice = text_to_speech(text)\n",
        "    display(play_audio(voice, autoplay=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0lxs2F1FfHq",
        "colab_type": "text"
      },
      "source": [
        "We can now call `hear` to speak into the mic, transfer our speech to words and predict the intent that we mean using our LUIS app."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJvuSon1wRlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# say something, the function will predict your intention\n",
        "intent = hear()\n",
        "# speak out loud the result\n",
        "speak(intent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbmrUhWhFa5I",
        "colab_type": "text"
      },
      "source": [
        "### Using your creation\n",
        "\n",
        "Let's write a function to trigger a set of actions based on the predicted/recognized intent of our LUIS model.\n",
        "In a sentence: let's define a function to execute what happens when a certain intent is predicted. (I'm sure you can come up with **many** ways to experiment at this step).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyIbPPRDF9PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Notebot():\n",
        "  '''Executes a set of actions based on what you say'''\n",
        "  # greeting message\n",
        "  speak('Hi there!')\n",
        "  # sleep while sound is being played\n",
        "  time.sleep(1)\n",
        "  clear_output()\n",
        "  \n",
        "  def run():\n",
        "    speak('What can I do for you?')\n",
        "    time.sleep(2)\n",
        "    # record voice, transform to text and predict the intention\n",
        "    intent = hear()\n",
        "    # carry out the predicted intent\n",
        "    if intent == 'describe':\n",
        "      speak(\"Alright, take a picture and I will describe it for you\")\n",
        "      img = take_picture('photo.jpg')\n",
        "      clear_output()\n",
        "      description = describe(img, localfile=True)\n",
        "      print(description)\n",
        "      speak('That looks like ' + description.strip())\n",
        "\n",
        "    elif intent == 'see':\n",
        "      speak(\"Alright, take a picture and I'll see what objects I can find\")\n",
        "      img = take_picture('photo.jpg')\n",
        "      clear_output()\n",
        "      # recognize objects \n",
        "      objects = see(img, localfile=True)\n",
        "      # making up results into a sentence\n",
        "      result = 'I can see '\n",
        "      for i in range(len(objects)):\n",
        "        if len(objects) > 1 and i == len(objects) - 1:\n",
        "          result = result[:-2] + f' and a {objects[i]}.'\n",
        "        else:\n",
        "          result += f'a {objects[i]}, '\n",
        "      # speak out loud the results\n",
        "      if len(objects) == 0:\n",
        "        result = \"Sorry I couldn't recognize anything\"\n",
        "      speak(result)\n",
        "\n",
        "    elif intent == 'detect_faces':\n",
        "      speak(\"Alright, take a picture and I'll find any humans\")\n",
        "      img = take_picture('photo.jpg')\n",
        "      clear_output()\n",
        "      detect_face(img, localfile=True)\n",
        "\n",
        "    elif intent == \"read\":\n",
        "      speak(\"Alright, put a document in front of me and I'll read it for you\")\n",
        "      img = take_picture('photo.jpg')\n",
        "      clear_output()\n",
        "      text = read(img, localfile=True)\n",
        "      result = ' '.join(text).replace('\\n', ' ')\n",
        "      if result == '':\n",
        "        speak(\"Hmm I couldn't find any text to read\")\n",
        "      speak(result)\n",
        "\n",
        "    # if no intent can be recognized, intent will be None\n",
        "    else:\n",
        "      speak(\"Sorry I could not understand you\")\n",
        "      time.sleep(2)\n",
        "      clear_output()\n",
        "      run()\n",
        "\n",
        "  run()\n",
        "  return \"I hope that was useful\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a68nFZA2GuT9",
        "colab_type": "text"
      },
      "source": [
        "To finalilze, let's summon the *Notebot* to fulfill our wishes. Depending on what you say, the `Notebot` can take a picture and:\n",
        "- speak out loud a description\n",
        "- display any detected objects\n",
        "- display any detected faces.\n",
        "- apply OCR and read out loud the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gLLdEzPntGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Notebot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TipYkB8r-XVG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Let's sum up what happens when you call it. At the beginning you will hear a greeting message. After that the Notebot will apply the function `hear` and start recording anything you say, your speech (the percept) will be transcribed to words and sent to the LUIS application to predict the intention that you have. Based on this prediction a different set of actions will be executed. In case there is no clear intent recognized from your speech, the intent \"None\" will be predicted and the `Notebot` will call itself again.\n",
        "\n",
        "\n",
        "\n",
        "Looked from above, the `Notebot` ends up acting as a simple reflex based agent, that simply finds a rule whose condition match the current situation and executes it. (In this case what the Notebot does if you say this or something else).  At this point you might like to upgrade your agent with additional concepts, e.g adding memory about what is perceived. But I'll leave that task for the diligent reader.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA3jIXio7J7g",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TPk0LGQ_X_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "resourceGroup=MyGroup\n",
        "az group delete --name $resourceGroup --yes\n",
        "rm -f keys.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOlX8Gn39rut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def delete_luis_app(app_id):\n",
        "  headers = {\n",
        "      # Request headers\n",
        "      'Ocp-Apim-Subscription-Key': authoring_key,\n",
        "  }\n",
        "\n",
        "  params = urllib.parse.urlencode({\n",
        "      # Request parameters\n",
        "      'force': 'false',\n",
        "  })\n",
        "\n",
        "  try:\n",
        "      conn = http.client.HTTPSConnection('westeurope.api.cognitive.microsoft.com')\n",
        "      conn.request(\"DELETE\", f\"/luis/api/v2.0/apps/{app_id}?%s\" % params, \"\", headers)\n",
        "      response = conn.getresponse()\n",
        "      data = response.read()\n",
        "      print(data)\n",
        "      conn.close()\n",
        "  except Exception as e:\n",
        "      print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU1blpuc-EPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "delete_luis_app(app_id)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}